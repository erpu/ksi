\documentclass[9pt,a4paper]{extarticle}

%\usepackage{abstract}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb} 
	%\usepackage{apacite}
%\usepackage{apl}
	%\usepackage[polski]{babelbib}
	%\usepackage{bibtopic}
	%\usepackage[
	%backend=biber,
	%style=alphabetic,
	%sorting=ynt
	%]{biblatex}
\usepackage[utf8]{inputenc}
\usepackage[nottoc,numbib]{tocbibind}

\usepackage{cite}
\usepackage{latexsym}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage{multicol}
%\usepackage{graphicx} 
	%\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
	%\usepackage{hyperref}
%\usepackage{indentfirst}

	%\usepackage{makeidx}
%\usepackage{natbib}

%\usepackage{setspace}
	%\usepackage{tabulary}
	%\usepackage[nottoc]{tocbibind}

	%\setlength{\parindent}{3pt}

\title{Web Page Classifier Information System\\ as an information software system\\ reducing Internet access\\ using keywords with wages}
\author{Piotr Wojcik\\
    Polish-Japanese Academy,\\
    Warsaw,\\
    \texttt{pwojcik@pja.edu.pl}\\
\\
    Krzysztof Stencel\\
    Warsaw University,\\
    Warsaw,\\
    \texttt{stecel@mimuw.edu.pl}}
\date{\today}


\begin{document}
%\frenchspacing
\maketitle
\bibliographystyle{plunsrt}
%\cleardoublepage

%\tableofcontents
%\clearpage

\section{Abstract}
Article describes proposition of new method of scanning Internet resources to find unsafe web pages for Internet users. Actual it is only an plan for the prototype that needs to be measured for his performance and usability. It is using concept of keywords with wages to classify or even selected censorship access to Internet thanks to list of blocked Internet web pages on dynamic updating Domain Names Servers.\\
\\
Keywords: \textit{Internet, classificator, classification, keywords with wage, Domain Names System, DNS}

\begin{multicols}{2}
\section{Introduction}
Almost every indexed Internet web page can by classified using proper \textbf{keywords}. Analysing appearance of keywords used on page known also as \textbf{search phrase} can tell about age requirements of Internet surfer for selected web page. Some phrases can tell the rule engine directly others can only guide for proper category. For example if on page exist word "nazism" there is a high probability that page is not appropriate for the fourteen years old or younger pearson. The classification of this type can be described as \textbf{direct rule}. Other phrases only guide us for proper classification. Let's think about phrase "Adolf Hitler". There is still huge probability that web page contains text about nazism, but also can lead to page about Adolf Hitlers biography that can by suitable for high school students.

That face us to the important problem of proper phrases classification that appears on web pages. Let's think about word "breast", which could lead us to erotic or even pornographic pages. But when we add another keyword and find material about "breast cancer" we can find medical pages about common disease that can be discussed also by high school students. However this time adding another keyword can lead us to very different pages in comparison to phrase "Adolf Hitler". We would named it \textbf{not direct rule}. This forces us to careful keywords grouping and establishing proper phrase wages to distinguish direct from non direct rules of comparison.

Classification could be done by artificial neural networks which can distinguish pages with nudity on images or videos\cite{will_archer_arentz_classifying_nodate, radhouane_guermazi_combining_2007, giuseppe_amato_detection_2009}. This technique of finding percent of coverage of skin color is very popular and gives good results in Internet media classification\cite{mohammad_reza_mahmoodi_high_nodate}. However this topic will not be discussed in this article.

There is other unpopular concept connected to this material and it is called Internet censorship. It was discussed in Jonathan Zittrain, Benjamin Edelman article\cite{jonathan_zittrain_internet_2003}. However more and more none regime countries like Australia, Great Britain and United States of America are using this controversial technique. The main reason of this idea is to choose lesser of two evils principle when there comes to talk about safety of younger Internet users\cite{piotr_luczuk_cyberwojna_2016}.

\section{DNS}
The best and also the most safe form to limit access to discover world wide web pages is to prepare appropriate Domain Names System DNS\cite{j._postel_domain_1984, p._mockapetris_domain_1987, p._mockapetris_domain2_1987, yakov_rekhter_dynamic_1997, r._elz_clarifications_1997, d._eastlake_domain_1999}.

Of course there is a easy way to bypass this security feature and not search for domain name IP address by directly typing this number. This leads us to necessity to remember this addresses. In IP protocol version four we encounter to \({256}^{4}\) different numbers with it is huge number of potential addresses. But in upcoming IP protocol version six we could address unthinkable number \(3.4*10^{1038}\). In practice those numbers we should reduce by number of sub networks of addresses classes that not lead directly to web pages content.

Another way of bypass this security feature is to manual replacing DNS server address in protected computer but even this step requires administration privileges.

\end{multicols}

%\lstinputlisting[language=c]
\lstdefinelanguage{csharp} {morekeywords={foreach, if, in, like}, sensitive=false, morecomment=[l]{//}, morecomment=[s]{/*}{*/}, morestring=[b]", }
\lstset{
numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt
}
\begin{lstlisting}[title={Alg 1. The basic Internet web page classifier algorithm}, language=csharp, label=alg1] 

//input: 
//    phrase - analysed phrase, 
//    analysedPage - web page with URL and other characteristics, 
//    compromisedWebPages - colection of compromised web pages, 
//    redFlagPhraseDictionary - colection of phrases 
//                              that compromise web page
//output:
//    score - points of page classification
//    compromisedWebPages - modyfied input object

mostCompromisedPages = compromisedWebPages.top();
foreach (phrase in webPageTextContent)
{  
  if((phrase in redFlagPhraseDictionary) 
      || (phrase like redFlagPhraseDictionary))
  {
    redFlagPhraseDictionary.computeWage(phrase, mostCompromisedPages);
    score += redFlagPhraseDictionary.wage(phrase);
    compromisedWebPages += analysedPage;
  }
}
\end{lstlisting}

\begin{multicols}{2}
\section{Keywords with wages}
Under the concept of \textbf{keywords with wage} are simple idea of extending semantic meaning of the word that are written for example with use of Latin alphabet letters by number value. For example word "sex" that may function as a verb it function also as a noun with five different meaning according to Dictionary.com\cite{noauthor_dictionary.com_2018}. This complicates a little direct classification because having this one world we cannot decide with great probability if the page is safe so we might want to estimate it \textbf{dangerous factor} \(\omega\) as equal 0.6. Let's define \textbf{keyword with wage} \(K\) as:
%\begin{math}
$$
K = \{a_1a_2a_3...a_n, \omega : a_n \in [A-Z], 
$$
$$
n \in \mathbb{N}, \omega \in \textless0,1\textgreater\}.
$$
%\end{math} 
The larger is dangerous factor \(\omega\) the more valuable is keyword because it is more efficient in classifying page as unsafe. In Red Flag Dictionaries described in algorithm [alg\ref{alg1}] we are expecting to include those keyword. 

\section{Keywords wages update and searching for new valuable keywords}
One of the characteristic of language is that it is dynamically changing and changing over the years.  More and more words are appearing. Some of them are become obsolete. This rises issue to constantly update a dictionary and wages of dangerous factor. Other fact is that similar pages use similar phrases in content. Traversing base set of pages can provides as not only an updated of wages but also can give us new set of phrases. In search of new phrases is applied more complex process than updating dangerous factor in existing set of keywords.

For the brief understanding of dynamic update dangerous factor parameter and classification of Internet pages I present simple algorithm [Alg1] for future analysis and implementation. Above code is implemented in C\# like language. The only not compatible feature is an \textbf{like} keyword that is not implemented in that language. This function is implemented in 4GL languages like SQL and can be easily replaced by calling method \textit{like()}. The body of this function may for example contain \textit{Soundex} algorithm created by Robert Russell and Margaret Odell\cite{donald_e._knuth_art_2002}. The result of this algorithm is four-digit code containing information about phonetic similarity of two words.

Idea of this algorithm depends on analysis of all \textit{phrases} on given Internet page \textit{analysedPage} to find any similarities 
with keywords in set \textit{redFlagPhraseDictionary} (line 15) called \textbf{red flag phrases}. If phrase is similar or identical then:
\begin{enumerate}
\item update all dangerous factor parameters in \textit{RedFlagPhraseDictionary} classification set and update the ranking static class \textit{MostCompromisedPages} (line 18),
\item sum up the \textit{score} for this page (line 19),
\item attach this page to list of compromised pages (line 20).
\end{enumerate}
On the line 12 we are setting up the top list of the most compromised pages \textit{compromisedWebPages.top()} as a source included in every iteration for this algorithm.

\section{Web pages category}
We may propose grouping of an Internet web pages in 12 different categories:
\begin{itemize}
\item information pages,
\item web databases,
\item commercial pages (shops, e-business etc.),
\item social networks,
\item dating sites,
\item erotic pages,
\item pornographic pages,
\item risk sites (for example online casinos),
\item sites with hate speech,
\item sites with illegal materials and
\item sites with hate and violence.
\end{itemize}
This might be not a final list o World Wide Web categorization and maybe requires an extension. Because of context of this pages the best strategy to deal with classification is to use different dictionaries, parameters or even algorithms to more efficient score decision. In case of classify site as a unsafe proper description should be added to database for future analysis and performance issues.

\section{Web Page Classification Information System modules}
The Web Page Classification Information System WPCIS is going to be build with several databases and use several different algorithms to process information.

First module contains database with all discovered Internet Pages that going to be process, was visited by one or many different classification algorithm, classified page or fully classified page with all external link processed. In case of classified or fully classified page description will be attached containing information which keywords was used to compute score of this page, the value of dangerous factor parameters used to make such index and phrases enumeration on this pages.

Second module are going to be build with much smaller database which contains perspectives of actual and used in the past sets of dictionaries of phrases with wages. Each perspective are going to have date of introduction, version number and used dictionary values and may be withdraw at any time in case of new parameters introduction. This database is a heart of system and going to be heavily updated by processing program.

Third, the smallest but important module is a program containing the algorithm to compute dangerous factor parameters for red flag phrases.

Fourth important module is a creator and updater of DNS server which use base of first module data. In some cases classification of pages could change so it is important to give this change possibility for DNS records.

Fifth the most complicated and resource consumer module is data-mining program that extracts new phrases for red flag dictionary algorithms.

Of course this system is going to grow in time and some modules are going to be divided with smaller components with more specialized features like four module that going to be divided with base for different DNS servers grouped by different pages categories described in section Web pages category.

\section{Web Page Classification Information System appliance}
First and the most obvious appliance of this classifier is home environment where young children and adolescent requires safe Internet access. Second also important place to use this system is in work environment and educational places where it is important to provide reduced access to some Internet resources.

\section{Conclusion and future plans}
The reason of this article appearance is to design new classification method. In near future new system are going to be implemented. The most important product of this program is to create plenty of DNS servers containing records of censured web domains according to the outcome of various classification algorithms. This DNS systems when applied to endpoint computers are going to greatly reduce access to unsafe or prohibited web resources.


%\begin{figure}
%\centering
%\includegraphics[width=\textwidth]{eegsignal.png}
%\includegraphics{eegsignal.png}
%\caption{Example of aquired EEG signal from Emotiv device}
%\label{fig:eegsignal}
%\end{figure}

%At the begining we are going to focus on signals similar to those presented in Figure \ref{fig:eegsignal}.


\bibliography{ksi}
\end{multicols}



\end{document}
