\documentclass[9pt,a4paper]{extarticle}

%\usepackage{abstract}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb} 
	%\usepackage{apacite}
%\usepackage{apl}
	%\usepackage[polski]{babelbib}
	%\usepackage{bibtopic}
	%\usepackage[
	%backend=biber,
	%style=alphabetic,
	%sorting=ynt
	%]{biblatex}
\usepackage[utf8]{inputenc}
\usepackage[nottoc,numbib]{tocbibind}

\usepackage{cite}
\usepackage{latexsym}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage{multicol}
%\usepackage{graphicx} 
	%\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
	%\usepackage{hyperref}
%\usepackage{indentfirst}

	%\usepackage{makeidx}
%\usepackage{natbib}

%\usepackage{setspace}
	%\usepackage{tabulary}
	%\usepackage[nottoc]{tocbibind}

	%\setlength{\parindent}{3pt}

\title{Web Page Classifier Information System\\ as an information software system\\ reducing Internet access\\ by usage of keywords\\ related with 'weight'}
\author{Piotr W\'{o}jcik\\
    Polish-Japanese Academy,\\
    Warsaw,\\
    \texttt{pwojcik@pja.edu.pl}
}
\date{\today}


\begin{document}

\maketitle
\bibliographystyle{plunsrt}

\section{Abstract}
This article describes a proposal of a new method to scan Internet resources for pages that are unsafe for selected Internet users. 
Currently, it is a plan for a prototype that will be evaluated for performance and usability. 
The method uses the concept of weighted keywords to classify and then even censor web pages.
The system will block selected Internet web pages by updating Domain Names System.\\
\\
Keywords: \textit{Internet, classifier, classification, keywords with wage, Domain Names System, DNS}

\begin{multicols}{2}
\section{Introduction}
Almost every indexed related to web page can be classified by usage of particular \textbf{keywords}. Analysing appearance of this keywords used on page known also as \textbf{search phrase} can tell about age requirements of Internet surfer for selected web page. Some phrases sets can tell the rule engine directly, others can only guide for proper category. For instance if on page appear word ``Nazism'' there is a high probability that page is not appropriate for the fourteen year olds or youngers. The classification of this type can be described as \textbf{direct rule}. Other phrases only guide us for adequate classification. Let's think about phrase ``Adolf Hitle''r. There is still huge probability that web page contains text about Nazism, but also can lead to page about Adolf Hitlers biography that can be  suitable for high school students.

This leave us with important problem, dysfunction of proper phrases classification that appears on web pages. Let's think about word ``breast'', which could lead us to erotic or even pornographic pages. But when we add another keyword and find material about ``breast cancer'' we can find medical pages about common disease that can be discussed also by high school students. However this time adding another keyword can lead us to very different pages in comparison to phrase ``Adolf Hitler''. We would call it \textbf{not direct rule}. This forces us to careful selection of keywords like proper phrase weight to distinguish direct form of non direct rules of comparison.

Classification could be done by artificial neural networks which can distinguish pages with nudity on images or videos\cite{will_archer_arentz_classifying_nodate, radhouane_guermazi_combining_2007, giuseppe_amato_detection_2009}. The technique of image extraction and accurate skin detection from web pages is very popular and gives good results in Internet media classification\cite{mohammad_reza_mahmoodi_high_nodate,paul_greenfield_netalert_2001} but in this article I won't focus on that topic. 

On the other hand there is unpopular concept connected to this material and  is called Internet Censorship. It was discussed in Jonathan Zittrain, Benjamin Edelman\cite{jonathan_zittrain_internet_2003} none regime countries like Australia, Great Britain and United States of America are constantly using this controversial technique. The main reason of this idea is to choose lesser of two evils which principle comes down to safety of young Internet surfers\cite{piotr_luczuk_cyberwojna_2016,john_g._palfrey_jr._four_2010}.

\section{DNS}
The best as well as the most safe way to limit access to web pages is basicly to prepare appropriate Domain Name System - DNS.
\cite{j._postel_domain_1984, p._mockapetris_domain_1987, p._mockapetris_domain2_1987, yakov_rekhter_dynamic_1997, r._elz_clarifications_1997, d._eastlake_3rd_domain_1997,donald_e._eastlake_3rd_dns_2000,brian_wellington_secure_2000,brian_wellington_domain_2000,edward_lewis_dns_2001}. Technique discussed in this article is called \textbf{the Exclusion Filtering}\cite{paul_greenfield_netalert_2001}.

Of course there is an easy way to bypass all this security features and simply not search for domain name IP address by directly typing this number. This leads us to necessity of  remembering that  addresses. In IP protocol version four we encounter to \({256}^{4}\) different numbers which  is huge number of potential addresses but in upcoming IP protocol version six we could address unthinkable number \(3.4*10^{1038}\). In practice those big numbers we should reduce by number of sub networks of address classes that not lead directly to web page.

Another way of bypass this security feature is to manually replacing DNS server address in the protected computer but even this step requires administrative privileges.
\end{multicols}

%\lstinputlisting[language=c]
\lstdefinelanguage{csharp} {morekeywords={foreach, if, in, like}, sensitive=false, morecomment=[l]{//}, morecomment=[s]{/*}{*/}, morestring=[b]", }
\lstset{
numbers=left, numberstyle=\tiny, stepnumber=1, numbersep=5pt
}
\begin{lstlisting}[title={Alg 1. The basic Internet web page classifier algorithm}, language=csharp, label=alg1] 

//input: 
//    phrase - analysed phrase, 
//    analysedPage - web page with URL and other characteristics, 
//    compromisedWebPages - collection of compromised web pages, 
//    redFlagPhraseDictionary - collection of phrases 
//                              that compromise web page
//output:
//    score - points of page classification
//    compromisedWebPages - modified input object

mostCompromisedPages = compromisedWebPages.top();
foreach (phrase in webPageTextContent)
{  
  if((phrase in redFlagPhraseDictionary) 
      || (phrase like redFlagPhraseDictionary))
  {
    redFlagPhraseDictionary.computeWage(phrase, mostCompromisedPages);
    score += redFlagPhraseDictionary.wage(phrase);
    compromisedWebPages += analysedPage;
  }
}
\end{lstlisting}

\begin{multicols}{2}
\section{Keywords with weight}
Concept of \textbf{keywords with weight} is a simple idea of extending semantic meaning of the word that is written in text, For instance usage of Latin alphabet letters by number value. To add more complications let analyze for example word sex which may function as a verb or function as a noun with five different meaning according to Dictionary.com\cite{noauthor_dictionary.com_2018}. This complicate direct classification because having one world we cannot decide with correct probability if the page is safe so we might want to estimate \textbf{dangerous factor} \(\omega\) as equal 0.6. Lets define \textbf{keyword with weight} \(K\) as as:
%\begin{math}
$$
K = \{a_1a_2a_3...a_n, \omega : a_n \in [A-Z], 
$$
$$
n \in \mathbb{N}, \omega \in \textless0,1\textgreater\}.
$$
%\end{math} 
The bigger is dangerous factor\(\omega\) the more valuable keyword is because is it more efficient in classifying page as unsafe. In Red Flag Dictionaries described in algorithm [alg\ref{alg1} we are expecting to include those keywords.

\section{Keywords weight update and searching for new valuable keywords}
Main characteristics of a language is that, that  is dynamically changing. More  new words appearing or slogan words are used. Some of them  become obsolete which rises issue to constantly update a dictionary and weight of dangerous factors \(\omega\).

Other fact is that similar pages use similar phrases in content. Traversing base set of pages can provides as not only an updated of weights but also can give us new set of phrases. In search of new phrases is applied more complex process than updating dangerous factors in existing set of keywords.

For the brief understanding of dynamic, update of dangerous factors \(\omega\) parameters and classification of Internet pages I present this simple algorithm [Alg1] for future analysis and implementation. Above code is implemented in C\#\cite{andrew_troelsen_jezyk_2011} like language. The only not compatible feature is \textbf{like} keyword that is not implemented in that language. This function is implemented in 4GL languages like SQL and can be easily replaced by calling method \textit{like()}. The body of this function may for example contain \textit{Soundex} algorithm created by Robert Russell and Margaret Odell\cite{donald_e._knuth_art_2002}. The result of this algorithm is four-digit code containing information about phonetic similarity of two given on input words.

Idea of this algorithm depends on analysis of all \textit{phrases} on given Internet page \textit{analysedPage} to find any similarities 
with keywords in set \textit{redFlagPhraseDictionary} (line 15) called \textbf{red flag phrases}. If phrase is similar or identical then:
\begin{enumerate}
\item update all dangerous factor parameters in \textit{RedFlagPhraseDictionary} classification set and update the ranking static class \textit{MostCompromisedPages} (line 18),
\item sum up the \textit{score} for this page (line 19),
\item attach this page to list of compromised pages (line 20).
\end{enumerate}
On the line 12 we are setting up the top list of the most compromised pages \textit{compromisedWebPages.top()} as a source included in every iteration for this algorithm.

\section{Web pages category}
We may propose grouping of an Internet web pages in 12 different categories:
\begin{itemize}
\item information pages,
\item web databases,
\item commercial pages (shops, e-business etc.),
\item social networks,
\item religious pages,
\item dating sites,
\item erotic pages,
\item pornographic pages,
\item risk sites (for example online casinos),
\item sites with hate speech,
\item sites with illegal materials and
\item sites with hate and violence.
\end{itemize}
This might not be  the final list of World Wide Web categorisation and extension may required. Because of the context of pages the best current strategy is to deal with classification how to use different dictionaries, parameters or even algorithms to more efficient score decision. In case of classified  site as  unsafe, proper description should be added to database for future analysis and performance issues.

\section{Web Page Classification Information System modules}
The proposed Web Page Classification Information System WPCIS is going to be built  with several databases and required  several different algorithms to process information.

First module contains database with all discovered Internet Pages that going to be process, was visited by one or many different classification algorithm, classified page or fully classified page with all external link processed. In case of classified or fully classified page description will be attached containing information which keywords was used to compute score of this page, the value of dangerous factors parameters used to make such index and phrases enumeration on this pages.

The Second module is going to be build with much smaller database containing perspectives of actual and used in the past sets of dictionaries and phrases with weight. Each perspective are going to have date of introduction, version number and dictionary values and may be withdraw at any time in case of new parameters introduction. This database is a heart of system and going to be heavily updated by processing program.

Third, the smallest but still important module is a program containing the algorithm to compute dangerous factors \(\omega\) parameters for red flag phrases.

Fourth important module is a creator and updater of DNS server which use base of first module data. In some cases classification of pages could change so it is important to give this change possibility for DNS records.

Fifth the most complicated and resources consumer module is data-mining program that extracts new phrases for red flag dictionary algorithms.

Of course this system is going to grow in time and some modules are going to be divided with smaller components with more specialized features like fourth module which is  going to be divided with base for different DNS servers grouped by different pages categories described in section Web pages category.

\section{Web Page Classification Information System appliance}
First and the most obvious place to use this appliance is home environment where young children and adolescent require safe Internet access. Second also important place to use the system is in work environment and educational places like schools, museums where it is important to provide reduced access to some Internet resources..

\section{Conclusion and future plans}
The reason of this article appearance is to design new classification method. In near future new systems are going to be implemented. The most important feature  of this program is to create plenty of DNS servers containing records of censured web domains depending on the outcome of various classification algorithms. This DNS systems can be applied  to endpoint of computers and are going to greatly reduce access to unsafe or prohibited web resources.

\bibliography{ksi}
\end{multicols}
\end{document}
